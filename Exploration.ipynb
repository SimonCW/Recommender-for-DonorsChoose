{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "0f141228-177f-4285-a531-b058d803a428"
    }
   },
   "source": [
    "# Project Recommendations for DonorsChoose\n",
    "\n",
    "This notebook explores different approaches to helping [DonorsChoose](https://www.donorschoose.org/) recommend the right project to right user.\n",
    "So far we have tried the following methods:\n",
    "- Content-based recommendations using tfidf\n",
    "    - tfidf project descriptions\n",
    "    - Calcualte document distances (e.g. cosine similarity)\n",
    "    - Explore document similarity\n",
    "    - Create recommendations based on similarity \n",
    "    - Create evaluation metric to test whether similarity is a good predictor for recommendations\n",
    "        - E.g. For users with #donations > 1 omit last donation, get ranking based on similar projects, check the ranking score of actually donated (omitted) projects\n",
    "        - Compare score agains popularity \"algorithm\" performance and \"recommending distinct projects\" or \"random projects\"\n",
    "- ...\n",
    "\n",
    "Other relevant methods:\n",
    "- Topic models using LDA\n",
    "- Tag generation (automated tagging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "ee5005c8-2abe-4eb3-aa2a-b6b9334fcfc0"
    }
   },
   "source": [
    "### Links\n",
    "\n",
    "- Nice tfidf helper code: https://towardsdatascience.com/hacking-scikit-learns-vectorizers-9ef26a7170af\n",
    "- We started doing approximately the content based recommender from here: https://www.kaggle.com/ranliu/donor-project-matching-with-recommender-systems/code (with more code but different challenge here: https://www.kaggle.com/gspmoreira/recommender-systems-in-python-101/code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b14c345c-fee2-496c-880e-470683a8f658"
    }
   },
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "694a0b72-918b-49b0-ae21-277c71ecd58a"
    }
   },
   "source": [
    "## Prerequisites\n",
    "To install spacy and download the English language model run: `\n",
    "conda install -c conda-forge spacy` and `python -m spacy download en`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbpresent": {
     "id": "ed3e213a-355e-4faf-b37f-8e4c2c95f9ef"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from spacy import lemmatizer, displacy\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "fb2eb4a2-f516-4bc9-91fd-61a22f9454f8"
    }
   },
   "source": [
    "### Load and trim data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbpresent": {
     "id": "a5b4d78f-ebce-48e0-8577-5d96063cedc7"
    }
   },
   "outputs": [],
   "source": [
    "# Test flag for faster exploration\n",
    "test_mode = True\n",
    "\n",
    "# I am reading in the cleaned projects csv from https://www.kaggle.com/madaha/cleaning-projects-csv-file\n",
    "projects = pd.read_csv(os.path.join(os.getcwd(), \"data\", \"projects_cleaned.csv\"),\n",
    "                       parse_dates=[\"Project_Posted_Date\", \"Project_Fully_Funded_Date\"])\n",
    "\n",
    "if test_mode:\n",
    "    projects = projects.head(100)\n",
    "\n",
    "def clean_text_cols(text_features=[\"Project_Title\", \"Project_Essay\"]):\n",
    "    \"\"\"Cleans the columns in text_features. Returns a dataframe\"\"\"\n",
    "    for feature in text_features:\n",
    "        projects.loc[:,feature] = projects.loc[:,feature].astype(str).fillna(\"\")\n",
    "        #projects.loc[:,feature] = projects.loc[:,feature].str.lower()\n",
    "    return projects \n",
    "\n",
    "projects = clean_text_cols(projects)\n",
    "\n",
    "text = projects[\"Project_Title\"] + \" \" + projects[\"Project_Essay\"] # This is a pandas series containing title and essay text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"what sound is happier than a ukulele?  we have students who can't wait to spend their lunch period strumming away, learning to play via on-line tutorials and creating student-led music education in our small junior high. our students are diverse and our school is rural. our kids are also entrepreneurial!  we have had many student-initiated projects on campus, and this project itself was created by a student with a passion for ukulele who wants to teach and learn with his peers. music education is hard to come by and expensive in our small, rural town.  with instruments to play, our students will have the chance to learn the basics of music, in a social setting (a club that meets at lunch) while learning about pacific island culture. these 4 ukuleles will become a part of our little junior high campus and will remain a part of our curriculum beyond my own tenure.  we try to engender an environment that promotes student-led learning, and i am excited to see students seeking musical enrichment in a age of dying fine arts education. \""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "projects.loc[1, \"Project_Essay\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = text.apply(lambda x: nlp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5\">Ukuleles For \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Middle Schoolers\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " what sound is happier than a ukulele?  we have students who can't wait to spend their lunch period strumming away, learning to play via on-line tutorials and creating student-led music education in our small junior high. our students are diverse and our school is rural. our kids are also entrepreneurial!  we have had many student-initiated projects on campus, and this project itself was created by a student with a passion for ukulele who wants to teach and learn with his peers. music education is hard to come by and expensive in our small, rural town.  with instruments to play, our students will have the chance to learn the basics of music, in a social setting (a club that meets at lunch) while learning about pacific island culture. these \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    4\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " ukuleles will become a part of our little junior high campus and will remain a part of our curriculum beyond my own tenure.  we try to engender an environment that promotes student-led learning, and i am excited to see students seeking musical enrichment in a age of dying fine arts education. </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(docs[1], style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "6b34347d-7de4-41c6-9b6e-fdd882a00152"
    }
   },
   "source": [
    "### Tfidf Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "nbpresent": {
     "id": "d1324311-b20d-4e9f-b098-72d5d4f04a1f"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create spacy tokenzier\n",
    "lemmatizer = spacy.lang.en.English()\n",
    "def spacy_tokenizer(doc):\n",
    "    tokens = lemmatizer(doc)\n",
    "    return([token.lemma_ for token in tokens])\n",
    "# Vectorize with custom spacy token lemmatizer, e.g. written, writing --> write\n",
    "\n",
    "vectorizer = TfidfVectorizer(strip_accents=\"unicode\",\n",
    "                            tokenizer=spacy_tokenizer,\n",
    "                            analyzer=\"word\",\n",
    "                            stop_words=\"english\",\n",
    "                            max_df=0.9,\n",
    "                            norm=\"l2\")\n",
    "\n",
    "\n",
    "project_ids = projects['Project_ID'].tolist()\n",
    "tfidf_matrix = vectorizer.fit_transform(text)\n",
    "tfidf_feature_names = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the most similar projects to a specific project of interest, I calculate the cosine similarity between that project and all others and return the ones with the highest cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar_cosines(tfidf_matrix, index, top_n = 5):\n",
    "    '''\n",
    "    tfidf_matrix, index document of interest -> list of tuples (index, cosine similarity)\n",
    "    '''\n",
    "    # Since the vectors have already been l2-normalized in the tfidfvectorizer a simple dot product suffices \n",
    "    # to calculate the cosine similarity. Use index +1 to converse rank ((5,1) instead of (5,))\n",
    "    cosine_similarities = linear_kernel(tfidf_matrix[index:index + 1], tfidf_matrix).flatten()\n",
    "    # Get indices for documents with highest cosine similarity. \n",
    "    related_docs_indices = (idx for idx in cosine_similarities.argsort()[::-1] if idx != index)\n",
    "    return [(index, cosine_similarities[index]) for index in list(related_docs_indices)[:top_n]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(88, 0.2565553027750274),\n",
       " (38, 0.17036385345475594),\n",
       " (25, 0.138652443261203),\n",
       " (90, 0.13727128990539694),\n",
       " (22, 0.12279547037657573)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top-5 most similar projects to the project with index 1\n",
    "similar_cosines(tfidf_matrix, 1, top_n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'have you ever tried to picture something new in your head without ever seeing it in person? the students in my school have been learning about instruments that kids play around the world, and now it is time for them to have the opportunity to play them! my students are extremely musical! they love to sing songs and play instruments, and by the time their 45-minute music class is over, they are moaning and groaning for more time to play. my students truly believe that music is a universal language that everyone in the world is able to speak. they are very interested in how children from all over the world make music. we have learned many songs from many countries, and my students have even been able to sing in other languages! the tactile aspect of being able to touch and play an instrument from a different part of the world is really amazing! since the students have learned about and had the opportunity to play instruments in the modern-day orchestra, this is a fantastic way for them to explore the world of multicultural instruments. by being able to pick up these instruments and play them, i will be able to reach all types of learners by incorporating the visual, audio, and tactile factors into my world instrument lessons. for the students in my school, my belief is that they need to have certain experiences in music class. i have opened their ears to sounds that they have never experienced before,= and taken them on journeys across the world with different pictures, recordings and videos of world instruments. by allowing the children in my school the opportunity to hold a different part of the world in their hands, you too will have opened their ears to the sweet sounds of music! '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "projects.loc[88, \"Project_Essay\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better understand the results, I extract the the top words (or rather tokens or features) for a document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_tfidf_features(tfidf_matrix, index, feature_names, top_n=5):\n",
    "    row = tfidf_matrix[index].toarray().flatten()\n",
    "    top_features = [(feature_names[idx], row[idx]) for idx in row.argsort()[::-1][:top_n]]\n",
    "    return top_features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ukulele', 0.3670846855190913),\n",
       " ('ukuleles', 0.36668457197241855),\n",
       " ('music', 0.3625657958762883),\n",
       " ('play', 0.3232747433279028),\n",
       " (' ', 0.2676539798559057)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_tfidf_features(tfidf_matrix, 1, tfidf_feature_names, top_n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently the lemmatier didn't stem ukuleles to ukulele and both appear as a token/feature. In general, the data needs much more cleaning, e.g. removing all the special character sequences such as `\\r\\n`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\t',\n",
       " '\\t\\t',\n",
       " '\\t\\t\\t',\n",
       " '\\t\\t\\t\\t',\n",
       " '\\t ',\n",
       " '\\t  \\t',\n",
       " '\\x10catapults',\n",
       " ' ',\n",
       " '  ',\n",
       " '   ',\n",
       " '    ',\n",
       " '     ',\n",
       " '      ',\n",
       " '       ',\n",
       " '        ',\n",
       " '         ',\n",
       " '          ',\n",
       " '           ',\n",
       " '            ',\n",
       " '             ',\n",
       " '              ',\n",
       " '                ',\n",
       " '                 ',\n",
       " '                  ',\n",
       " '                    ',\n",
       " '                     ',\n",
       " '                       ',\n",
       " '                        ',\n",
       " '                            ',\n",
       " '                                 ',\n",
       " '                                    ',\n",
       " '                                     ',\n",
       " '                                             ',\n",
       " '                                                 ',\n",
       " '                                                      ',\n",
       " '                                                                                                                                                                                                       ',\n",
       " '                                                                                                                                                                                                                                                                             ',\n",
       " '                                                                                                                                                                                                                                                                                                                                                                                                                                                   ',\n",
       " '!',\n",
       " '#',\n",
       " '$',\n",
       " '$0.00.which',\n",
       " '$4.49',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " \"''\",\n",
       " \"'isaac\",\n",
       " \"'normal\",\n",
       " \"'s\",\n",
       " \"'text\",\n",
       " \"'~isaac\",\n",
       " '(',\n",
       " '(and',\n",
       " '(cicero',\n",
       " '(do',\n",
       " '(how',\n",
       " '(need',\n",
       " '(o:',\n",
       " '(shift',\n",
       " '(working',\n",
       " ')',\n",
       " ')keeping',\n",
       " ')now',\n",
       " ')of',\n",
       " ')tape',\n",
       " '*',\n",
       " '+',\n",
       " ',-',\n",
       " ',---',\n",
       " ',i',\n",
       " '-',\n",
       " '-(18inch',\n",
       " '-).writing',\n",
       " '-)writing',\n",
       " '--',\n",
       " '---',\n",
       " '----',\n",
       " '-----',\n",
       " '-------',\n",
       " '----marian',\n",
       " '---bill',\n",
       " '---dr',\n",
       " '---effective',\n",
       " '---john',\n",
       " '---laura',\n",
       " '---marva',\n",
       " '---students',\n",
       " '---with',\n",
       " '--aesop',\n",
       " '--aesopwhat',\n",
       " '--albert',\n",
       " '--all',\n",
       " '--alma',\n",
       " '--amy',\n",
       " '--and',\n",
       " '--archimedesshape',\n",
       " '--arthur',\n",
       " '--atticus',\n",
       " '--author',\n",
       " '--believe',\n",
       " '--bill',\n",
       " '--both',\n",
       " '--by',\n",
       " '--c',\n",
       " '--cesar',\n",
       " '--charles',\n",
       " '--confuciusthey',\n",
       " '--coolio',\n",
       " '--david',\n",
       " '--do',\n",
       " '--dr',\n",
       " '--edward',\n",
       " '--ernest',\n",
       " '--fifth',\n",
       " '--frederick',\n",
       " '--hands',\n",
       " '--heidi',\n",
       " '--helping',\n",
       " '--henry',\n",
       " \"--i'm\",\n",
       " '--jennifer',\n",
       " '--john',\n",
       " '--karl',\n",
       " '--kofi',\n",
       " '--ludwig',\n",
       " '--mahatma',\n",
       " '--mark',\n",
       " '--maya',\n",
       " '--michelle',\n",
       " '--movies',\n",
       " '--mr',\n",
       " '--ms',\n",
       " '--nctm',\n",
       " '--nelson',\n",
       " '--no',\n",
       " '--not',\n",
       " '--one',\n",
       " '--or',\n",
       " '--plato',\n",
       " '--plutarch',\n",
       " '--roald',\n",
       " '--ruth',\n",
       " '--stories',\n",
       " '--students',\n",
       " '--that',\n",
       " '--the',\n",
       " '--these',\n",
       " '--to',\n",
       " '--topics',\n",
       " '--track',\n",
       " '--unable',\n",
       " '--william',\n",
       " '-1',\n",
       " '-10',\n",
       " '-11',\n",
       " '-12',\n",
       " '-12th',\n",
       " '-15',\n",
       " '-18',\n",
       " '-19',\n",
       " '-1st',\n",
       " '-2',\n",
       " '-2014',\n",
       " '-22',\n",
       " '-24',\n",
       " '-29',\n",
       " '-3',\n",
       " '-30',\n",
       " '-32',\n",
       " '-35',\n",
       " '-3rd',\n",
       " '-4',\n",
       " '-40',\n",
       " '-4th',\n",
       " '-5',\n",
       " '-5th',\n",
       " '-6',\n",
       " '-60',\n",
       " '-6th',\n",
       " '-70',\n",
       " '-7th',\n",
       " '-8',\n",
       " '-8th',\n",
       " '-9',\n",
       " '-PRON-',\n",
       " '-a',\n",
       " '-a-',\n",
       " '-a.a',\n",
       " '-abbamusic',\n",
       " '-abraham',\n",
       " '-action',\n",
       " '-affluent',\n",
       " '-african',\n",
       " '-age',\n",
       " '-aged',\n",
       " '-ajwe',\n",
       " '-al',\n",
       " '-alan',\n",
       " '-albert',\n",
       " '-alfred',\n",
       " '-alice',\n",
       " '-all',\n",
       " '-along',\n",
       " '-american',\n",
       " '-an',\n",
       " '-and',\n",
       " '-android',\n",
       " '-andy',\n",
       " '-angela',\n",
       " '-anis',\n",
       " '-anna',\n",
       " '-anne',\n",
       " '-anonymous;reading',\n",
       " '-anonymousand',\n",
       " '-anonymousbut',\n",
       " '-anonymousdevelopers',\n",
       " '-ansel',\n",
       " '-antoine',\n",
       " '-any',\n",
       " '-april',\n",
       " '-archimedes',\n",
       " '-aristotle',\n",
       " '-aristotleespecially',\n",
       " '-arizona',\n",
       " '-art',\n",
       " '-arthur',\n",
       " '-articulating',\n",
       " '-as',\n",
       " '-at',\n",
       " '-atwood',\n",
       " '-author',\n",
       " '-autism',\n",
       " '-autistic',\n",
       " '-awareness',\n",
       " '-a\\x80a',\n",
       " '-a\\x80ajacqueline',\n",
       " '-a\\x80a\\x95',\n",
       " \"-a\\x83a\\x83a\\x82a\\x82a\\x83a\\x82'french\",\n",
       " '-a\\x95',\n",
       " '-b.b',\n",
       " '-b.f',\n",
       " '-ballard',\n",
       " '-barbara',\n",
       " '-baron',\n",
       " '-based',\n",
       " '-ben',\n",
       " '-benjamin',\n",
       " '-besides',\n",
       " '-bette',\n",
       " '-bill',\n",
       " '-bo',\n",
       " '-bob',\n",
       " '-bobby',\n",
       " '-bonowith',\n",
       " '-both',\n",
       " '-brad',\n",
       " '-bud',\n",
       " '-buddha',\n",
       " '-building',\n",
       " '-but',\n",
       " '-by',\n",
       " '-c',\n",
       " '-c.k.h',\n",
       " '-c.s',\n",
       " '-cameron',\n",
       " '-carl',\n",
       " '-cbnow',\n",
       " '-ccss',\n",
       " '-cecil',\n",
       " '-cesar',\n",
       " '-charles',\n",
       " '-cheryl',\n",
       " '-chinese',\n",
       " '-chuck',\n",
       " '-cicero',\n",
       " '-city',\n",
       " '-cjwe',\n",
       " '-clarence',\n",
       " '-clarke',\n",
       " '-class',\n",
       " '-claudette',\n",
       " '-clay',\n",
       " '-computer',\n",
       " '-confucius',\n",
       " '-confucius.meaningful',\n",
       " '-confuciusmany',\n",
       " '-confuciusmath',\n",
       " '-connie',\n",
       " '-cooking',\n",
       " '-coyne',\n",
       " '-creating',\n",
       " '-curricular',\n",
       " '-cvwe',\n",
       " '-d',\n",
       " '-dalai',\n",
       " '-dale',\n",
       " '-dan',\n",
       " '-daniel',\n",
       " '-david',\n",
       " '-day',\n",
       " '-defense',\n",
       " '-determined',\n",
       " '-developing',\n",
       " '-dewey',\n",
       " '-digital',\n",
       " '-dimitri',\n",
       " '-dioum.a',\n",
       " '-donald',\n",
       " '-douglass',\n",
       " '-dr',\n",
       " '-dr.seuss',\n",
       " '-dr.seusssuch',\n",
       " '-driven',\n",
       " '-drumming',\n",
       " '-dry',\n",
       " '-due',\n",
       " '-dyslexic',\n",
       " '-e',\n",
       " '-e.m',\n",
       " '-easy',\n",
       " '-eberhard',\n",
       " '-economic',\n",
       " '-ed',\n",
       " '-edgar',\n",
       " '-edwin',\n",
       " '-eileen',\n",
       " '-einstein',\n",
       " '-einstein.collaborative',\n",
       " '-elbert',\n",
       " '-eleanor',\n",
       " '-elizabeth',\n",
       " '-elliot',\n",
       " '-emilie',\n",
       " '-ernest',\n",
       " '-escaping',\n",
       " '-especially',\n",
       " '-esteem',\n",
       " '-euripidesfocused',\n",
       " '-even',\n",
       " '-every',\n",
       " '-everything',\n",
       " '-excellence',\n",
       " '-exploring',\n",
       " '-family',\n",
       " '-fellow',\n",
       " '-fidgeter',\n",
       " '-fifth',\n",
       " '-francis',\n",
       " '-frank',\n",
       " '-fransis',\n",
       " '-franz',\n",
       " '-fred',\n",
       " '-frederick',\n",
       " '-from',\n",
       " '-gabriela',\n",
       " '-galileo',\n",
       " '-gandhi',\n",
       " '-gandhiproviding',\n",
       " '-garrison',\n",
       " '-gavin',\n",
       " '-general',\n",
       " '-george',\n",
       " '-georgia',\n",
       " '-giving',\n",
       " '-goethe',\n",
       " '-greg',\n",
       " '-growing',\n",
       " '-hand',\n",
       " '-harper',\n",
       " '-harriet',\n",
       " '-havens',\n",
       " '-haves',\n",
       " '-having',\n",
       " '-hazel',\n",
       " '-headstart',\n",
       " '-hear',\n",
       " '-helen',\n",
       " '-help',\n",
       " '-henry',\n",
       " '-high',\n",
       " '-hirsch',\n",
       " '-homelessness',\n",
       " '-homework',\n",
       " '-honest-',\n",
       " '-hot',\n",
       " '-how',\n",
       " '-howard',\n",
       " '-hrwe',\n",
       " '-i',\n",
       " '-ignacio',\n",
       " '-increasing',\n",
       " '-inspire',\n",
       " '-ious',\n",
       " '-ipads',\n",
       " '-ippilito',\n",
       " '-isaac',\n",
       " '-isaacmarion.com',\n",
       " '-it',\n",
       " '-ivan',\n",
       " '-ivar',\n",
       " '-jacob',\n",
       " '-jacqueline',\n",
       " '-jacques',\n",
       " '-jane',\n",
       " '-janelle',\n",
       " '-jean',\n",
       " '-jennifer',\n",
       " '-jfk',\n",
       " '-jim',\n",
       " '-jimi',\n",
       " '-johann',\n",
       " '-john',\n",
       " '-jorge',\n",
       " '-joyce',\n",
       " '-jules',\n",
       " '-julian',\n",
       " '-just',\n",
       " '-k',\n",
       " '-karl',\n",
       " '-katherine',\n",
       " '-kelly',\n",
       " '-ken',\n",
       " '-kmwe',\n",
       " '-kofi',\n",
       " \"-l'il\",\n",
       " '-language',\n",
       " '-lao',\n",
       " '-laplace',\n",
       " '-laptop',\n",
       " '-lasting',\n",
       " '-latin',\n",
       " '-lawrence',\n",
       " '-learning',\n",
       " '-lee',\n",
       " '-lemony',\n",
       " '-leo',\n",
       " '-leonardo',\n",
       " '-let',\n",
       " '-level',\n",
       " '-like',\n",
       " '-lily',\n",
       " '-linda',\n",
       " '-linguistic',\n",
       " '-listening',\n",
       " '-literacy',\n",
       " '-long',\n",
       " '-lou',\n",
       " '-lucy',\n",
       " '-maclifethe',\n",
       " '-magicians',\n",
       " '-mahatma',\n",
       " '-major',\n",
       " '-malcolm',\n",
       " '-management',\n",
       " '-margaret',\n",
       " '-maria',\n",
       " '-marian',\n",
       " '-marianne',\n",
       " '-marie',\n",
       " '-mark',\n",
       " '-marlee',\n",
       " '-martin',\n",
       " '-marva',\n",
       " '-mary',\n",
       " '-mason',\n",
       " '-math.there',\n",
       " '-matt',\n",
       " '-maya',\n",
       " '-maybe',\n",
       " '-mcp',\n",
       " '-michael',\n",
       " '-michelle',\n",
       " '-mickey',\n",
       " '-montessori',\n",
       " '-morning',\n",
       " '-multicultural',\n",
       " '-my',\n",
       " '-n',\n",
       " '-n.l.g',\n",
       " '-nancy',\n",
       " '-napoleon',\n",
       " '-native',\n",
       " '-nelson',\n",
       " '-new',\n",
       " '-nicholas',\n",
       " '-no',\n",
       " '-nominated',\n",
       " '-norman',\n",
       " '-not',\n",
       " '-nspire',\n",
       " '-of',\n",
       " '-old',\n",
       " '-oliver',\n",
       " '-olsmart',\n",
       " '-on',\n",
       " '-one',\n",
       " '-oprah',\n",
       " '-or-',\n",
       " '-oriented',\n",
       " '-orson',\n",
       " '-oscar',\n",
       " '-other',\n",
       " '-our',\n",
       " '-out',\n",
       " '-p.d',\n",
       " '-pablo',\n",
       " '-page',\n",
       " '-pangaea',\n",
       " '-paper',\n",
       " '-part',\n",
       " '-paul',\n",
       " '-pearl',\n",
       " '-pearson',\n",
       " '-physically',\n",
       " '-plato',\n",
       " '-platomale',\n",
       " '-play',\n",
       " '-please',\n",
       " '-powerful',\n",
       " '-practice',\n",
       " '-preferably',\n",
       " '-president',\n",
       " '-priced',\n",
       " '-print',\n",
       " '-prompt',\n",
       " '-proverbdue',\n",
       " '-pythagoras',\n",
       " '-r.j',\n",
       " '-rachel',\n",
       " '-raffi',\n",
       " '-raising',\n",
       " '-ralph',\n",
       " '-rarely',\n",
       " '-rasmenia',\n",
       " '-ray',\n",
       " '-read',\n",
       " '-reading',\n",
       " '-regie',\n",
       " '-regular',\n",
       " '-related',\n",
       " '-rene',\n",
       " '-research',\n",
       " '-richard',\n",
       " '-right',\n",
       " '-risk',\n",
       " '-rita',\n",
       " '-roald',\n",
       " '-robert',\n",
       " '-ron',\n",
       " '-roughly',\n",
       " '-rumithen',\n",
       " '-run',\n",
       " '-s',\n",
       " '-s-',\n",
       " '-scholastic',\n",
       " '-scholasticthis',\n",
       " '-science',\n",
       " '-scott',\n",
       " '-second',\n",
       " '-senecaurban',\n",
       " '-share',\n",
       " '-shel',\n",
       " '-shelia',\n",
       " '-sherman',\n",
       " '-shoemaker',\n",
       " '-sir',\n",
       " '-sister',\n",
       " '-sixth',\n",
       " '-sized',\n",
       " '-small',\n",
       " '-so',\n",
       " '-socioeconomic',\n",
       " '-socrates',\n",
       " \"-socrates.i'd\",\n",
       " '-solving',\n",
       " '-sometimes',\n",
       " '-special',\n",
       " '-spence',\n",
       " '-staff',\n",
       " '-stans',\n",
       " '-step',\n",
       " '-stephanie',\n",
       " '-stephen',\n",
       " '-steve',\n",
       " '-students',\n",
       " '-susan',\n",
       " '-t',\n",
       " '-tagorea',\n",
       " '-take',\n",
       " '-teach',\n",
       " '-teachers',\n",
       " '-teaching',\n",
       " '-team',\n",
       " '-technology',\n",
       " '-temple',\n",
       " '-text',\n",
       " '-thank',\n",
       " '-that',\n",
       " '-the',\n",
       " '-then',\n",
       " '-theodore',\n",
       " '-there',\n",
       " '-these',\n",
       " '-they',\n",
       " '-thomas',\n",
       " '-threatening',\n",
       " '-three',\n",
       " '-tiny',\n",
       " '-to',\n",
       " '-tomie',\n",
       " '-trey',\n",
       " '-try',\n",
       " '-u.s',\n",
       " '-unknown',\n",
       " '-unknowndessert',\n",
       " '-up',\n",
       " '-us',\n",
       " '-used',\n",
       " '-value',\n",
       " '-vi',\n",
       " '-victor',\n",
       " '-vince',\n",
       " '-virtual',\n",
       " '-voltaireparents',\n",
       " '-w',\n",
       " '-w.b',\n",
       " '-walt',\n",
       " '-water',\n",
       " '-we',\n",
       " '-what',\n",
       " '-where',\n",
       " '-which',\n",
       " '-who',\n",
       " '-wide',\n",
       " '-will',\n",
       " '-william',\n",
       " '-winning',\n",
       " '-winston',\n",
       " '-with',\n",
       " '-woody',\n",
       " '-working',\n",
       " '-world',\n",
       " '-writing',\n",
       " '-year',\n",
       " '-yes',\n",
       " '-yet',\n",
       " '-yeuzheng',\n",
       " '-yoko',\n",
       " '-your',\n",
       " '-|',\n",
       " '.-',\n",
       " '.--',\n",
       " '.---',\n",
       " '.-drumming',\n",
       " '..',\n",
       " '...',\n",
       " '....',\n",
       " '.....',\n",
       " '......',\n",
       " '.......',\n",
       " '........',\n",
       " '.........',\n",
       " '..........',\n",
       " '...........',\n",
       " '............',\n",
       " '.............',\n",
       " '.01',\n",
       " '.04',\n",
       " '.05',\n",
       " '.1',\n",
       " '.18',\n",
       " '.20',\n",
       " '.22',\n",
       " '.25',\n",
       " '.3',\n",
       " '.4',\n",
       " '.45',\n",
       " '.5',\n",
       " '.583',\n",
       " '.6',\n",
       " '.7',\n",
       " '.75',\n",
       " '.82',\n",
       " '.95',\n",
       " '.a',\n",
       " '.absolutely',\n",
       " '.after',\n",
       " '.allowing',\n",
       " '.also',\n",
       " '.although',\n",
       " '.and',\n",
       " '.another',\n",
       " '.are',\n",
       " '.as',\n",
       " '.at',\n",
       " '.awesome',\n",
       " '.bee',\n",
       " '.but',\n",
       " '.by',\n",
       " '.challenging',\n",
       " '.clarence',\n",
       " '.distances',\n",
       " '.do',\n",
       " '.english',\n",
       " '.enrichment',\n",
       " '.find',\n",
       " '.finding',\n",
       " '.fitness',\n",
       " '.get',\n",
       " '.help',\n",
       " '.helping',\n",
       " '.hook',\n",
       " '.hopefully',\n",
       " '.i',\n",
       " '.in',\n",
       " '.inside',\n",
       " '.instead',\n",
       " '.is',\n",
       " '.it',\n",
       " '.jpeg',\n",
       " '.last',\n",
       " '.many',\n",
       " '.multimedia',\n",
       " '.my',\n",
       " '.not',\n",
       " '.now',\n",
       " '.oh',\n",
       " '.our',\n",
       " '.pdf',\n",
       " '.please',\n",
       " '.practicing',\n",
       " '.pro',\n",
       " '.put',\n",
       " '.reading',\n",
       " '.ready',\n",
       " '.resulting',\n",
       " '.reusable',\n",
       " '.school',\n",
       " '.seriously',\n",
       " '.share',\n",
       " '.since',\n",
       " '.speaker',\n",
       " '.stay',\n",
       " '.stories',\n",
       " '.struggle',\n",
       " '.students',\n",
       " '.that',\n",
       " '.the',\n",
       " '.these',\n",
       " '.they',\n",
       " '.this',\n",
       " '.through',\n",
       " '.unfortunately',\n",
       " '.values',\n",
       " '.videos',\n",
       " '.we',\n",
       " '.working',\n",
       " '.writing',\n",
       " '.~',\n",
       " '/',\n",
       " '/-',\n",
       " '/8th',\n",
       " '/a/',\n",
       " '/activity',\n",
       " '/adequate',\n",
       " '/ap',\n",
       " '/apple',\n",
       " '/at/',\n",
       " '/b/',\n",
       " '/c/',\n",
       " '/ch/).with',\n",
       " '/colored',\n",
       " '/contrast',\n",
       " '/d/',\n",
       " '/deaf',\n",
       " '/disney',\n",
       " '/e/',\n",
       " '/experience',\n",
       " '/f/',\n",
       " '/felt',\n",
       " '/further',\n",
       " '/if',\n",
       " '/is',\n",
       " '/lessons',\n",
       " '/m/',\n",
       " '/my',\n",
       " '/no/.',\n",
       " '/non',\n",
       " '/on/',\n",
       " '/or',\n",
       " '/p/',\n",
       " '/pe',\n",
       " '/personal',\n",
       " '/planets',\n",
       " '/point',\n",
       " '/qu/.',\n",
       " '/r/',\n",
       " '/r/,/s/',\n",
       " '/s.i.m.s',\n",
       " '/s/',\n",
       " '/science',\n",
       " '/sims',\n",
       " '/study',\n",
       " '/t/',\n",
       " '/t/.',\n",
       " '/t/?my',\n",
       " '/teacher',\n",
       " '/understanding',\n",
       " '/writing',\n",
       " '0',\n",
       " '0-',\n",
       " '0.001',\n",
       " '0.01',\n",
       " '0.04',\n",
       " '0.05',\n",
       " '0.1',\n",
       " '0.1-',\n",
       " '0.2',\n",
       " '0.20',\n",
       " '0.3',\n",
       " '0.35',\n",
       " '0.4',\n",
       " '0.5',\n",
       " '0.50',\n",
       " '0.58',\n",
       " '0.6',\n",
       " '0.6-',\n",
       " '0.60',\n",
       " '0.661',\n",
       " '0.7-',\n",
       " '0.75',\n",
       " '0.8',\n",
       " '0.85',\n",
       " '0.9',\n",
       " '0.95',\n",
       " '0/6',\n",
       " '00',\n",
       " '000',\n",
       " '001please',\n",
       " '00s',\n",
       " '01',\n",
       " '02',\n",
       " '03',\n",
       " '055x',\n",
       " '06',\n",
       " '08',\n",
       " '09',\n",
       " '0=3',\n",
       " '0f',\n",
       " '0we',\n",
       " '1',\n",
       " '1!one',\n",
       " '1)engage',\n",
       " '1+x=10',\n",
       " '1,000',\n",
       " '1,000,000',\n",
       " '1,000,000,000',\n",
       " '1,000s',\n",
       " '1,008',\n",
       " '1,011',\n",
       " '1,012',\n",
       " '1,025',\n",
       " '1,030',\n",
       " '1,033',\n",
       " '1,045',\n",
       " '1,050',\n",
       " '1,067',\n",
       " '1,076',\n",
       " '1,098',\n",
       " '1,100',\n",
       " '1,100(professional',\n",
       " '1,106',\n",
       " '1,125',\n",
       " '1,144',\n",
       " '1,150',\n",
       " '1,160',\n",
       " '1,169',\n",
       " '1,2',\n",
       " '1,2,3',\n",
       " '1,2,3,4',\n",
       " '1,2,3-read',\n",
       " '1,2,4,and',\n",
       " '1,200',\n",
       " '1,230',\n",
       " '1,240',\n",
       " '1,245',\n",
       " '1,250',\n",
       " '1,256',\n",
       " '1,260',\n",
       " '1,3,and',\n",
       " '1,300',\n",
       " '1,334',\n",
       " '1,350',\n",
       " '1,358',\n",
       " '1,361',\n",
       " '1,400',\n",
       " '1,440',\n",
       " '1,450',\n",
       " '1,470',\n",
       " '1,480',\n",
       " '1,5',\n",
       " '1,500',\n",
       " '1,500.we',\n",
       " '1,517',\n",
       " '1,551',\n",
       " '1,600',\n",
       " '1,637',\n",
       " '1,650',\n",
       " '1,700',\n",
       " '1,750',\n",
       " '1,800',\n",
       " '1,800,000',\n",
       " '1,820',\n",
       " '1,840',\n",
       " '1,900',\n",
       " '1,944',\n",
       " '1,950',\n",
       " '1-',\n",
       " '1-3.here',\n",
       " '1-5.they',\n",
       " '1-a',\n",
       " '1-bedroom',\n",
       " '1-create',\n",
       " '1-d',\n",
       " '1-hour',\n",
       " '1-inch',\n",
       " '1-informational',\n",
       " '1-mile',\n",
       " '1-minute',\n",
       " '1-music',\n",
       " '1-on-1',\n",
       " '1-parent',\n",
       " '1-piece',\n",
       " '1-point',\n",
       " '1-read',\n",
       " '1-to-1',\n",
       " '1-to-3',\n",
       " '1-year',\n",
       " '1.0',\n",
       " '1.00',\n",
       " '1.1',\n",
       " '1.16',\n",
       " '1.2',\n",
       " '1.2.3',\n",
       " '1.25',\n",
       " '1.3',\n",
       " '1.4',\n",
       " '1.48',\n",
       " '1.5',\n",
       " '1.5).i',\n",
       " '1.54',\n",
       " '1.5v',\n",
       " '1.6',\n",
       " '1.62',\n",
       " '1.66',\n",
       " '1.7',\n",
       " '1.7.1',\n",
       " '1.75',\n",
       " '1.77',\n",
       " '1.77%)so',\n",
       " '1.8',\n",
       " '1.8.1',\n",
       " '1.9',\n",
       " '1.community',\n",
       " '1.i',\n",
       " '1.my',\n",
       " '1.small',\n",
       " '1.the',\n",
       " '1.what',\n",
       " '1/',\n",
       " '1/10',\n",
       " '1/12',\n",
       " '1/16',\n",
       " '1/17',\n",
       " '1/2',\n",
       " '1/2-month',\n",
       " '1/2-size',\n",
       " '1/25',\n",
       " '1/2x11',\n",
       " '1/3',\n",
       " '1/3rd',\n",
       " '1/4',\n",
       " '1/4-size',\n",
       " '1/4th',\n",
       " '1/5',\n",
       " '1/5th',\n",
       " '1/6th',\n",
       " '1/7/13',\n",
       " '1/8',\n",
       " '1/8000s',\n",
       " '1/equity',\n",
       " '1/low',\n",
       " '1/program',\n",
       " '1/title',\n",
       " '10',\n",
       " '10!.',\n",
       " '10!i',\n",
       " '10%+',\n",
       " '10%-black',\n",
       " ...]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "a19b35a3-78f5-4122-8d45-bd6dde4e26ed"
    }
   },
   "source": [
    "# Unrelated Stuff / Testing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "f1df4d32-8b08-40eb-81fc-24d300191c87"
    }
   },
   "source": [
    "## Understanding tfidf with small examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "828f2b11-aa1a-4ccd-9586-215ae5a349cc"
    }
   },
   "outputs": [],
   "source": [
    "# Good intro with nice code: https://towardsdatascience.com/hacking-scikit-learns-vectorizers-9ef26a7170af\n",
    "# create a dataframe from a word matrix\n",
    "def wm2df(wm, feat_names):\n",
    "    \n",
    "    # create an index for each row\n",
    "    timeit doc_names = ['Doc{:d}'.format(idx) for idx, _ in enumerate(wm)]\n",
    "    df = pd.DataFrame(data=wm.toarray(), index=doc_names,\n",
    "                      columns=feat_names)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "b1a99f01-d40d-4bf1-a21f-396d548de816"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "test_series = pd.Series([\"I write a sentence about school's computer \", \"I've written a sentence about school furniture\", \"I am writing a sentence about school computer periphery\"])\n",
    "\n",
    "# smooth_idf adds one to every df to prevent division by \"0\"\n",
    "# Create spacy tokenzier\n",
    "spacy.load(\"en\")\n",
    "lemmatizer = spacy.lang.en.English()\n",
    "def spacy_tokenizer(doc):\n",
    "    tokens = lemmatizer(doc)\n",
    "    return([token.lemma_ for token in tokens])\n",
    "# Vectorize with custom spacy token lemmatizer, e.g. written, writing --> write\n",
    "test_tfidf = TfidfVectorizer(tokenizer=spacy_tokenizer, analyzer=\"word\", stop_words=\"english\", smooth_idf=False)\n",
    "test_fitted = test_tfidf.fit(test_series)\n",
    "test_transformed = test_fitted.transform(test_series)\n",
    "\n",
    "idf = test_tfidf.idf_\n",
    "\n",
    "print(\"> The vocabulary doesn't contain 'write' and 'written' as seperate tokens. '-PRON-' is a special token for pronouns\")\n",
    "print(test_tfidf.vocabulary_)\n",
    "print(\"\\n> 'Furniture' and periphery have the highest idf weights because they only appear in one document. 'Computer' appears in two out of three documents\")\n",
    "# Zip groups the elements from first object with elements from second object by index\n",
    "print(dict(zip(test_fitted.get_feature_names(), idf)))\n",
    "print(\"\\n> The tf-idf matrix\")\n",
    "print(wm2df(test_transformed, test_tfidf.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "377d5870-77c2-4ca0-9979-06eedb83490c"
    }
   },
   "source": [
    "## Nice performance evaluation tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "e38319eb-23d0-44a5-9021-085e7d44439b"
    }
   },
   "outputs": [],
   "source": [
    "def my_add():\n",
    "    return list(projects.loc[:, \"Teacher Project Posted Sequence\"] + 100)\n",
    "\n",
    "def iter_add():\n",
    "    lst = []\n",
    "    for row in projects.loc[:,\"Teacher Project Posted Sequence\"].values:\n",
    "        val = row + 100\n",
    "        lst.append(val)\n",
    "    return  lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "7e05d63e-39ea-4f9a-ae8a-5d147b7b012b"
    }
   },
   "outputs": [],
   "source": [
    "%timeit my_add()\n",
    "%timeit iter_add()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "nbpresent": {
     "id": "eb035902-8eb9-4db9-8cb7-0719668f9408"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "c696d017-d996-47cc-9cab-b564e7bfb9ac"
    }
   },
   "outputs": [],
   "source": [
    "%lprun -f my_add my_add()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "6d0c6b7f-1d25-4ea1-8d3b-5d5a3782f110"
    }
   },
   "outputs": [],
   "source": [
    "%lprun -f iter_add iter_add()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
