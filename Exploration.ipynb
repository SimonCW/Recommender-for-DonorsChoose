{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "0f141228-177f-4285-a531-b058d803a428"
    }
   },
   "source": [
    "# Project Recommendations for DonorsChoose\n",
    "\n",
    "This notebook explores different approaches to helping [DonorsChoose](https://www.donorschoose.org/) recommend the right project to right user.\n",
    "So far we have tried the following methods:\n",
    "- Content-based recommendations using tfidf\n",
    "    - tfidf project descriptions\n",
    "    - Calcualte document distances (e.g. cosine similarity)\n",
    "    - Explore document similarity\n",
    "    - Create recommendations based on similarity \n",
    "    - Create evaluation metric to test whether similarity is a good predictor for recommendations\n",
    "        - E.g. For users with #donations > 1 omit last donation, get ranking based on similar projects, check the ranking score of actually donated (omitted) projects\n",
    "        - Compare score agains popularity \"algorithm\" performance and \"recommending distinct projects\" or \"random projects\"\n",
    "- ...\n",
    "\n",
    "Other relevant methods:\n",
    "- Topic models using LDA\n",
    "- Tag generation (automated tagging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "ee5005c8-2abe-4eb3-aa2a-b6b9334fcfc0"
    }
   },
   "source": [
    "### Links\n",
    "\n",
    "- Nice tfidf helper code: https://towardsdatascience.com/hacking-scikit-learns-vectorizers-9ef26a7170af\n",
    "- We started doing approximately the content based recommender from here: https://www.kaggle.com/ranliu/donor-project-matching-with-recommender-systems/code (with more code but different challenge here: https://www.kaggle.com/gspmoreira/recommender-systems-in-python-101/code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b14c345c-fee2-496c-880e-470683a8f658"
    }
   },
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "694a0b72-918b-49b0-ae21-277c71ecd58a"
    }
   },
   "source": [
    "## Prerequisites\n",
    "To install spacy and download the English language model run: `\n",
    "conda install -c conda-forge spacy` and `python -m spacy download en`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbpresent": {
     "id": "ed3e213a-355e-4faf-b37f-8e4c2c95f9ef"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "fb2eb4a2-f516-4bc9-91fd-61a22f9454f8"
    }
   },
   "source": [
    "### Load and trim data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbpresent": {
     "id": "a5b4d78f-ebce-48e0-8577-5d96063cedc7"
    }
   },
   "outputs": [],
   "source": [
    "# Test flag for faster exploration\n",
    "test_mode = True\n",
    "\n",
    "# I am reading in the cleaned projects csv from https://www.kaggle.com/madaha/cleaning-projects-csv-file\n",
    "projects = pd.read_csv(os.path.join(os.getcwd(), \"data\", \"projects_cleaned.csv\"),\n",
    "                       parse_dates=[\"Project_Posted_Date\", \"Project_Fully_Funded_Date\"])\n",
    "\n",
    "if test_mode:\n",
    "    projects = projects.head(100000)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    100000.0\n",
       "mean         32.0\n",
       "std           0.0\n",
       "min          32.0\n",
       "25%          32.0\n",
       "50%          32.0\n",
       "75%          32.0\n",
       "max          32.0\n",
       "Name: Project_ID, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that all ids have length 32\n",
    "projects.loc[:, \"Project_ID\"].str.len().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "nbpresent": {
     "id": "79b25762-ce3c-4094-bc4d-5fca8ae78f7e"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Project_ID</th>\n",
       "      <th>School_ID</th>\n",
       "      <th>Teacher_ID</th>\n",
       "      <th>Teacher_Project_Posted_Sequence</th>\n",
       "      <th>Project_Type</th>\n",
       "      <th>Project_Title</th>\n",
       "      <th>Project_Essay</th>\n",
       "      <th>Project_Subject_Category_Tree</th>\n",
       "      <th>Project_Subject_Subcategory_Tree</th>\n",
       "      <th>Project_Grade_Level_Category</th>\n",
       "      <th>Project_Resource_Category</th>\n",
       "      <th>Project_Cost</th>\n",
       "      <th>Project_Posted_Date</th>\n",
       "      <th>Project_Current_Status</th>\n",
       "      <th>Project_Fully_Funded_Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>77b7d3f2ac4e32d538914e4a8cb8a525</td>\n",
       "      <td>c2d5cb0a29a62e72cdccee939f434181</td>\n",
       "      <td>59f7d2c62f7e76a99d31db6f62b7b67c</td>\n",
       "      <td>2</td>\n",
       "      <td>Teacher-Led</td>\n",
       "      <td>Anti-Bullying Begins with Me</td>\n",
       "      <td>do you remember your favorite classroom from e...</td>\n",
       "      <td>Applied Learning, Literacy &amp; Language</td>\n",
       "      <td>Character Education, Literacy</td>\n",
       "      <td>Grades PreK-2</td>\n",
       "      <td>Books</td>\n",
       "      <td>490.38</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>Fully Funded</td>\n",
       "      <td>2013-03-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fd928b7f6386366a9cad2bea40df4b25</td>\n",
       "      <td>8acbb544c9215b25c71a0c655200baea</td>\n",
       "      <td>8fbd92394e20d647ddcdc6085ce1604b</td>\n",
       "      <td>1</td>\n",
       "      <td>Teacher-Led</td>\n",
       "      <td>Ukuleles For Middle Schoolers</td>\n",
       "      <td>what sound is happier than a ukulele?  we have...</td>\n",
       "      <td>Music &amp; The Arts</td>\n",
       "      <td>Music</td>\n",
       "      <td>Grades 6-8</td>\n",
       "      <td>Supplies</td>\n",
       "      <td>420.61</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>Expired</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7c915e8e1d27f10a94abd689e99c336f</td>\n",
       "      <td>0ae85ea7c7acc41cffa9f81dc61d46df</td>\n",
       "      <td>9140ac16d2e6cee45bd50b0b2ce8cd04</td>\n",
       "      <td>2</td>\n",
       "      <td>Teacher-Led</td>\n",
       "      <td>Big Books, Flip Books, And Everything In Between</td>\n",
       "      <td>my 1st graders may be small, but they have big...</td>\n",
       "      <td>Literacy &amp; Language, Special Needs</td>\n",
       "      <td>Literacy, Special Needs</td>\n",
       "      <td>Grades PreK-2</td>\n",
       "      <td>Books</td>\n",
       "      <td>510.46</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>Fully Funded</td>\n",
       "      <td>2013-01-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>feeec44c2a3a3d9a31c137a9780d2521</td>\n",
       "      <td>deddcdb20f86599cefa5e7eb31da309b</td>\n",
       "      <td>63750e765b46f9fa4d71e780047e896e</td>\n",
       "      <td>1</td>\n",
       "      <td>Teacher-Led</td>\n",
       "      <td>A Little for a Lot</td>\n",
       "      <td>our students were so excited to come to school...</td>\n",
       "      <td>Math &amp; Science, Literacy &amp; Language</td>\n",
       "      <td>Applied Sciences, Literature &amp; Writing</td>\n",
       "      <td>Grades 3-5</td>\n",
       "      <td>Supplies</td>\n",
       "      <td>282.80</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>Fully Funded</td>\n",
       "      <td>2013-05-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>037719bf60853f234610458a210f45a9</td>\n",
       "      <td>f3f0dc60ba3026944eeffe8b76cb8d9c</td>\n",
       "      <td>0d5b4cc12b2eb00013460d0ac38ce2a2</td>\n",
       "      <td>1</td>\n",
       "      <td>Teacher-Led</td>\n",
       "      <td>Technology in the Classroom</td>\n",
       "      <td>were you ever that kid that struggled to stay ...</td>\n",
       "      <td>Literacy &amp; Language, Math &amp; Science</td>\n",
       "      <td>Literacy, Mathematics</td>\n",
       "      <td>Grades PreK-2</td>\n",
       "      <td>Technology</td>\n",
       "      <td>555.28</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>Fully Funded</td>\n",
       "      <td>2013-02-14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Project_ID                         School_ID  \\\n",
       "0  77b7d3f2ac4e32d538914e4a8cb8a525  c2d5cb0a29a62e72cdccee939f434181   \n",
       "1  fd928b7f6386366a9cad2bea40df4b25  8acbb544c9215b25c71a0c655200baea   \n",
       "2  7c915e8e1d27f10a94abd689e99c336f  0ae85ea7c7acc41cffa9f81dc61d46df   \n",
       "3  feeec44c2a3a3d9a31c137a9780d2521  deddcdb20f86599cefa5e7eb31da309b   \n",
       "4  037719bf60853f234610458a210f45a9  f3f0dc60ba3026944eeffe8b76cb8d9c   \n",
       "\n",
       "                         Teacher_ID  Teacher_Project_Posted_Sequence  \\\n",
       "0  59f7d2c62f7e76a99d31db6f62b7b67c                                2   \n",
       "1  8fbd92394e20d647ddcdc6085ce1604b                                1   \n",
       "2  9140ac16d2e6cee45bd50b0b2ce8cd04                                2   \n",
       "3  63750e765b46f9fa4d71e780047e896e                                1   \n",
       "4  0d5b4cc12b2eb00013460d0ac38ce2a2                                1   \n",
       "\n",
       "  Project_Type                                     Project_Title  \\\n",
       "0  Teacher-Led                      Anti-Bullying Begins with Me   \n",
       "1  Teacher-Led                     Ukuleles For Middle Schoolers   \n",
       "2  Teacher-Led  Big Books, Flip Books, And Everything In Between   \n",
       "3  Teacher-Led                                A Little for a Lot   \n",
       "4  Teacher-Led                       Technology in the Classroom   \n",
       "\n",
       "                                       Project_Essay  \\\n",
       "0  do you remember your favorite classroom from e...   \n",
       "1  what sound is happier than a ukulele?  we have...   \n",
       "2  my 1st graders may be small, but they have big...   \n",
       "3  our students were so excited to come to school...   \n",
       "4  were you ever that kid that struggled to stay ...   \n",
       "\n",
       "           Project_Subject_Category_Tree  \\\n",
       "0  Applied Learning, Literacy & Language   \n",
       "1                       Music & The Arts   \n",
       "2     Literacy & Language, Special Needs   \n",
       "3    Math & Science, Literacy & Language   \n",
       "4    Literacy & Language, Math & Science   \n",
       "\n",
       "         Project_Subject_Subcategory_Tree Project_Grade_Level_Category  \\\n",
       "0           Character Education, Literacy                Grades PreK-2   \n",
       "1                                   Music                   Grades 6-8   \n",
       "2                 Literacy, Special Needs                Grades PreK-2   \n",
       "3  Applied Sciences, Literature & Writing                   Grades 3-5   \n",
       "4                   Literacy, Mathematics                Grades PreK-2   \n",
       "\n",
       "  Project_Resource_Category  Project_Cost Project_Posted_Date  \\\n",
       "0                     Books        490.38          2013-01-01   \n",
       "1                  Supplies        420.61          2013-01-01   \n",
       "2                     Books        510.46          2013-01-01   \n",
       "3                  Supplies        282.80          2013-01-01   \n",
       "4                Technology        555.28          2013-01-01   \n",
       "\n",
       "  Project_Current_Status Project_Fully_Funded_Date  \n",
       "0           Fully Funded                2013-03-12  \n",
       "1                Expired                       NaT  \n",
       "2           Fully Funded                2013-01-07  \n",
       "3           Fully Funded                2013-05-29  \n",
       "4           Fully Funded                2013-02-14  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "projects.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "6b34347d-7de4-41c6-9b6e-fdd882a00152"
    }
   },
   "source": [
    "### Tfidf Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "nbpresent": {
     "id": "d1324311-b20d-4e9f-b098-72d5d4f04a1f"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_features = [\"Project_Title\", \"Project_Essay\"]\n",
    "\n",
    "# Preprocess text columns to make them comparable\n",
    "for feature in text_features:\n",
    "    projects.loc[:,feature] = projects.loc[:,feature].astype(str).fillna(\"\")\n",
    "    projects.loc[:,feature] = projects.loc[:,feature].str.lower()\n",
    "    \n",
    "text = projects[\"Project_Title\"] + \" \" + projects[\"Project_Essay\"] # This is a pandas series containing title and essay text\n",
    "\n",
    "# Create spacy tokenzier\n",
    "spacy.load(\"en\")\n",
    "lemmatizer = spacy.lang.en.English()\n",
    "def spacy_tokenizer(doc):\n",
    "    tokens = lemmatizer(doc)\n",
    "    return([token.lemma_ for token in tokens])\n",
    "# Vectorize with custom spacy token lemmatizer, e.g. written, writing --> write\n",
    "\n",
    "vectorizer = TfidfVectorizer(strip_accents=\"unicode\",\n",
    "                            tokenizer=spacy_tokenizer,\n",
    "                            analyzer=\"word\",\n",
    "                            stop_words=\"english\",\n",
    "                            max_df=0.9,\n",
    "                            norm=\"l2\")\n",
    "\n",
    "\n",
    "project_ids = projects['Project_ID'].tolist()\n",
    "tfidf_matrix = vectorizer.fit_transform(text)\n",
    "tfidf_feature_names = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the most similar projects to a specific project of interest, I calculate the cosine similarity between that project and all others and return the ones with the highest cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar_cosines(tfidf_matrix, index, top_n = 5):\n",
    "    '''\n",
    "    tfidf_matrix, index document of interest -> list of tuples (index, cosine similarity)\n",
    "    '''\n",
    "    # Since the vectors have already been l2-normalized in the tfidfvectorizer a simple dot product suffices \n",
    "    # to calculate the cosine similarity. Use index +1 to converse rank ((5,1) instead of (5,))\n",
    "    cosine_similarities = linear_kernel(tfidf_matrix[index:index + 1], tfidf_matrix).flatten()\n",
    "    # Get indices for documents with highest cosine similarity. \n",
    "    related_docs_indices = (idx for idx in cosine_similarities.argsort()[::-1] if idx != index)\n",
    "    return [(index, cosine_similarities[index]) for index in list(related_docs_indices)[:top_n]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(76076, 0.4583541366716915),\n",
       " (68223, 0.45650586618657546),\n",
       " (44429, 0.45010123244891087),\n",
       " (92507, 0.43961156488836367),\n",
       " (39100, 0.43817310591515995)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top-5 most similar projects to the project with index 1\n",
    "similar_cosines(tfidf_matrix, 1, top_n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making music come alive with ukuleles in the classroom! how many times have you heard someone say, i wish i knew how to play an instrument; or,  if only i had learned to read music.  the ability to read and play music is a skill (and a joy) that will last a lifetime.  teaching ukulele allows childre\n"
     ]
    }
   ],
   "source": [
    "print(text[76076][:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better understand the results, I extract the the top words (or rather tokens or features) for a document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_tfidf_features(tfidf_matrix, index, feature_names, top_n=5):\n",
    "    row = tfidf_matrix[index].toarray().flatten()\n",
    "    top_features = [(feature_names[idx], row[idx]) for idx in row.argsort()[::-1][:top_n]]\n",
    "    return top_features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ukulele', 0.3670846855190913),\n",
       " ('ukuleles', 0.36668457197241855),\n",
       " ('music', 0.3625657958762883),\n",
       " ('play', 0.3232747433279028),\n",
       " (' ', 0.2676539798559057)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_tfidf_features(tfidf_matrix, 1, tfidf_feature_names, top_n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently the lemmatier didn't stem ukuleles to ukulele and both appear as a token/feature. In general, the data needs much more cleaning, e.g. removing all the special character sequences such as `\\r\\n`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\t',\n",
       " '\\t\\t',\n",
       " '\\t\\t\\t',\n",
       " '\\t\\t\\t\\t',\n",
       " '\\t ',\n",
       " '\\t  \\t',\n",
       " '\\x10catapults',\n",
       " ' ',\n",
       " '  ',\n",
       " '   ',\n",
       " '    ',\n",
       " '     ',\n",
       " '      ',\n",
       " '       ',\n",
       " '        ',\n",
       " '         ',\n",
       " '          ',\n",
       " '           ',\n",
       " '            ',\n",
       " '             ',\n",
       " '              ',\n",
       " '                ',\n",
       " '                 ',\n",
       " '                  ',\n",
       " '                    ',\n",
       " '                     ',\n",
       " '                       ',\n",
       " '                        ',\n",
       " '                            ',\n",
       " '                                 ',\n",
       " '                                    ',\n",
       " '                                     ',\n",
       " '                                             ',\n",
       " '                                                 ',\n",
       " '                                                      ',\n",
       " '                                                                                                                                                                                                       ',\n",
       " '                                                                                                                                                                                                                                                                             ',\n",
       " '                                                                                                                                                                                                                                                                                                                                                                                                                                                   ',\n",
       " '!',\n",
       " '#',\n",
       " '$',\n",
       " '$0.00.which',\n",
       " '$4.49',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " \"''\",\n",
       " \"'isaac\",\n",
       " \"'normal\",\n",
       " \"'s\",\n",
       " \"'text\",\n",
       " \"'~isaac\",\n",
       " '(',\n",
       " '(and',\n",
       " '(cicero',\n",
       " '(do',\n",
       " '(how',\n",
       " '(need',\n",
       " '(o:',\n",
       " '(shift',\n",
       " '(working',\n",
       " ')',\n",
       " ')keeping',\n",
       " ')now',\n",
       " ')of',\n",
       " ')tape',\n",
       " '*',\n",
       " '+',\n",
       " ',-',\n",
       " ',---',\n",
       " ',i',\n",
       " '-',\n",
       " '-(18inch',\n",
       " '-).writing',\n",
       " '-)writing',\n",
       " '--',\n",
       " '---',\n",
       " '----',\n",
       " '-----',\n",
       " '-------',\n",
       " '----marian',\n",
       " '---bill',\n",
       " '---dr',\n",
       " '---effective',\n",
       " '---john',\n",
       " '---laura',\n",
       " '---marva',\n",
       " '---students',\n",
       " '---with',\n",
       " '--aesop',\n",
       " '--aesopwhat',\n",
       " '--albert',\n",
       " '--all',\n",
       " '--alma',\n",
       " '--amy',\n",
       " '--and',\n",
       " '--archimedesshape',\n",
       " '--arthur',\n",
       " '--atticus',\n",
       " '--author',\n",
       " '--believe',\n",
       " '--bill',\n",
       " '--both',\n",
       " '--by',\n",
       " '--c',\n",
       " '--cesar',\n",
       " '--charles',\n",
       " '--confuciusthey',\n",
       " '--coolio',\n",
       " '--david',\n",
       " '--do',\n",
       " '--dr',\n",
       " '--edward',\n",
       " '--ernest',\n",
       " '--fifth',\n",
       " '--frederick',\n",
       " '--hands',\n",
       " '--heidi',\n",
       " '--helping',\n",
       " '--henry',\n",
       " \"--i'm\",\n",
       " '--jennifer',\n",
       " '--john',\n",
       " '--karl',\n",
       " '--kofi',\n",
       " '--ludwig',\n",
       " '--mahatma',\n",
       " '--mark',\n",
       " '--maya',\n",
       " '--michelle',\n",
       " '--movies',\n",
       " '--mr',\n",
       " '--ms',\n",
       " '--nctm',\n",
       " '--nelson',\n",
       " '--no',\n",
       " '--not',\n",
       " '--one',\n",
       " '--or',\n",
       " '--plato',\n",
       " '--plutarch',\n",
       " '--roald',\n",
       " '--ruth',\n",
       " '--stories',\n",
       " '--students',\n",
       " '--that',\n",
       " '--the',\n",
       " '--these',\n",
       " '--to',\n",
       " '--topics',\n",
       " '--track',\n",
       " '--unable',\n",
       " '--william',\n",
       " '-1',\n",
       " '-10',\n",
       " '-11',\n",
       " '-12',\n",
       " '-12th',\n",
       " '-15',\n",
       " '-18',\n",
       " '-19',\n",
       " '-1st',\n",
       " '-2',\n",
       " '-2014',\n",
       " '-22',\n",
       " '-24',\n",
       " '-29',\n",
       " '-3',\n",
       " '-30',\n",
       " '-32',\n",
       " '-35',\n",
       " '-3rd',\n",
       " '-4',\n",
       " '-40',\n",
       " '-4th',\n",
       " '-5',\n",
       " '-5th',\n",
       " '-6',\n",
       " '-60',\n",
       " '-6th',\n",
       " '-70',\n",
       " '-7th',\n",
       " '-8',\n",
       " '-8th',\n",
       " '-9',\n",
       " '-PRON-',\n",
       " '-a',\n",
       " '-a-',\n",
       " '-a.a',\n",
       " '-abbamusic',\n",
       " '-abraham',\n",
       " '-action',\n",
       " '-affluent',\n",
       " '-african',\n",
       " '-age',\n",
       " '-aged',\n",
       " '-ajwe',\n",
       " '-al',\n",
       " '-alan',\n",
       " '-albert',\n",
       " '-alfred',\n",
       " '-alice',\n",
       " '-all',\n",
       " '-along',\n",
       " '-american',\n",
       " '-an',\n",
       " '-and',\n",
       " '-android',\n",
       " '-andy',\n",
       " '-angela',\n",
       " '-anis',\n",
       " '-anna',\n",
       " '-anne',\n",
       " '-anonymous;reading',\n",
       " '-anonymousand',\n",
       " '-anonymousbut',\n",
       " '-anonymousdevelopers',\n",
       " '-ansel',\n",
       " '-antoine',\n",
       " '-any',\n",
       " '-april',\n",
       " '-archimedes',\n",
       " '-aristotle',\n",
       " '-aristotleespecially',\n",
       " '-arizona',\n",
       " '-art',\n",
       " '-arthur',\n",
       " '-articulating',\n",
       " '-as',\n",
       " '-at',\n",
       " '-atwood',\n",
       " '-author',\n",
       " '-autism',\n",
       " '-autistic',\n",
       " '-awareness',\n",
       " '-a\\x80a',\n",
       " '-a\\x80ajacqueline',\n",
       " '-a\\x80a\\x95',\n",
       " \"-a\\x83a\\x83a\\x82a\\x82a\\x83a\\x82'french\",\n",
       " '-a\\x95',\n",
       " '-b.b',\n",
       " '-b.f',\n",
       " '-ballard',\n",
       " '-barbara',\n",
       " '-baron',\n",
       " '-based',\n",
       " '-ben',\n",
       " '-benjamin',\n",
       " '-besides',\n",
       " '-bette',\n",
       " '-bill',\n",
       " '-bo',\n",
       " '-bob',\n",
       " '-bobby',\n",
       " '-bonowith',\n",
       " '-both',\n",
       " '-brad',\n",
       " '-bud',\n",
       " '-buddha',\n",
       " '-building',\n",
       " '-but',\n",
       " '-by',\n",
       " '-c',\n",
       " '-c.k.h',\n",
       " '-c.s',\n",
       " '-cameron',\n",
       " '-carl',\n",
       " '-cbnow',\n",
       " '-ccss',\n",
       " '-cecil',\n",
       " '-cesar',\n",
       " '-charles',\n",
       " '-cheryl',\n",
       " '-chinese',\n",
       " '-chuck',\n",
       " '-cicero',\n",
       " '-city',\n",
       " '-cjwe',\n",
       " '-clarence',\n",
       " '-clarke',\n",
       " '-class',\n",
       " '-claudette',\n",
       " '-clay',\n",
       " '-computer',\n",
       " '-confucius',\n",
       " '-confucius.meaningful',\n",
       " '-confuciusmany',\n",
       " '-confuciusmath',\n",
       " '-connie',\n",
       " '-cooking',\n",
       " '-coyne',\n",
       " '-creating',\n",
       " '-curricular',\n",
       " '-cvwe',\n",
       " '-d',\n",
       " '-dalai',\n",
       " '-dale',\n",
       " '-dan',\n",
       " '-daniel',\n",
       " '-david',\n",
       " '-day',\n",
       " '-defense',\n",
       " '-determined',\n",
       " '-developing',\n",
       " '-dewey',\n",
       " '-digital',\n",
       " '-dimitri',\n",
       " '-dioum.a',\n",
       " '-donald',\n",
       " '-douglass',\n",
       " '-dr',\n",
       " '-dr.seuss',\n",
       " '-dr.seusssuch',\n",
       " '-driven',\n",
       " '-drumming',\n",
       " '-dry',\n",
       " '-due',\n",
       " '-dyslexic',\n",
       " '-e',\n",
       " '-e.m',\n",
       " '-easy',\n",
       " '-eberhard',\n",
       " '-economic',\n",
       " '-ed',\n",
       " '-edgar',\n",
       " '-edwin',\n",
       " '-eileen',\n",
       " '-einstein',\n",
       " '-einstein.collaborative',\n",
       " '-elbert',\n",
       " '-eleanor',\n",
       " '-elizabeth',\n",
       " '-elliot',\n",
       " '-emilie',\n",
       " '-ernest',\n",
       " '-escaping',\n",
       " '-especially',\n",
       " '-esteem',\n",
       " '-euripidesfocused',\n",
       " '-even',\n",
       " '-every',\n",
       " '-everything',\n",
       " '-excellence',\n",
       " '-exploring',\n",
       " '-family',\n",
       " '-fellow',\n",
       " '-fidgeter',\n",
       " '-fifth',\n",
       " '-francis',\n",
       " '-frank',\n",
       " '-fransis',\n",
       " '-franz',\n",
       " '-fred',\n",
       " '-frederick',\n",
       " '-from',\n",
       " '-gabriela',\n",
       " '-galileo',\n",
       " '-gandhi',\n",
       " '-gandhiproviding',\n",
       " '-garrison',\n",
       " '-gavin',\n",
       " '-general',\n",
       " '-george',\n",
       " '-georgia',\n",
       " '-giving',\n",
       " '-goethe',\n",
       " '-greg',\n",
       " '-growing',\n",
       " '-hand',\n",
       " '-harper',\n",
       " '-harriet',\n",
       " '-havens',\n",
       " '-haves',\n",
       " '-having',\n",
       " '-hazel',\n",
       " '-headstart',\n",
       " '-hear',\n",
       " '-helen',\n",
       " '-help',\n",
       " '-henry',\n",
       " '-high',\n",
       " '-hirsch',\n",
       " '-homelessness',\n",
       " '-homework',\n",
       " '-honest-',\n",
       " '-hot',\n",
       " '-how',\n",
       " '-howard',\n",
       " '-hrwe',\n",
       " '-i',\n",
       " '-ignacio',\n",
       " '-increasing',\n",
       " '-inspire',\n",
       " '-ious',\n",
       " '-ipads',\n",
       " '-ippilito',\n",
       " '-isaac',\n",
       " '-isaacmarion.com',\n",
       " '-it',\n",
       " '-ivan',\n",
       " '-ivar',\n",
       " '-jacob',\n",
       " '-jacqueline',\n",
       " '-jacques',\n",
       " '-jane',\n",
       " '-janelle',\n",
       " '-jean',\n",
       " '-jennifer',\n",
       " '-jfk',\n",
       " '-jim',\n",
       " '-jimi',\n",
       " '-johann',\n",
       " '-john',\n",
       " '-jorge',\n",
       " '-joyce',\n",
       " '-jules',\n",
       " '-julian',\n",
       " '-just',\n",
       " '-k',\n",
       " '-karl',\n",
       " '-katherine',\n",
       " '-kelly',\n",
       " '-ken',\n",
       " '-kmwe',\n",
       " '-kofi',\n",
       " \"-l'il\",\n",
       " '-language',\n",
       " '-lao',\n",
       " '-laplace',\n",
       " '-laptop',\n",
       " '-lasting',\n",
       " '-latin',\n",
       " '-lawrence',\n",
       " '-learning',\n",
       " '-lee',\n",
       " '-lemony',\n",
       " '-leo',\n",
       " '-leonardo',\n",
       " '-let',\n",
       " '-level',\n",
       " '-like',\n",
       " '-lily',\n",
       " '-linda',\n",
       " '-linguistic',\n",
       " '-listening',\n",
       " '-literacy',\n",
       " '-long',\n",
       " '-lou',\n",
       " '-lucy',\n",
       " '-maclifethe',\n",
       " '-magicians',\n",
       " '-mahatma',\n",
       " '-major',\n",
       " '-malcolm',\n",
       " '-management',\n",
       " '-margaret',\n",
       " '-maria',\n",
       " '-marian',\n",
       " '-marianne',\n",
       " '-marie',\n",
       " '-mark',\n",
       " '-marlee',\n",
       " '-martin',\n",
       " '-marva',\n",
       " '-mary',\n",
       " '-mason',\n",
       " '-math.there',\n",
       " '-matt',\n",
       " '-maya',\n",
       " '-maybe',\n",
       " '-mcp',\n",
       " '-michael',\n",
       " '-michelle',\n",
       " '-mickey',\n",
       " '-montessori',\n",
       " '-morning',\n",
       " '-multicultural',\n",
       " '-my',\n",
       " '-n',\n",
       " '-n.l.g',\n",
       " '-nancy',\n",
       " '-napoleon',\n",
       " '-native',\n",
       " '-nelson',\n",
       " '-new',\n",
       " '-nicholas',\n",
       " '-no',\n",
       " '-nominated',\n",
       " '-norman',\n",
       " '-not',\n",
       " '-nspire',\n",
       " '-of',\n",
       " '-old',\n",
       " '-oliver',\n",
       " '-olsmart',\n",
       " '-on',\n",
       " '-one',\n",
       " '-oprah',\n",
       " '-or-',\n",
       " '-oriented',\n",
       " '-orson',\n",
       " '-oscar',\n",
       " '-other',\n",
       " '-our',\n",
       " '-out',\n",
       " '-p.d',\n",
       " '-pablo',\n",
       " '-page',\n",
       " '-pangaea',\n",
       " '-paper',\n",
       " '-part',\n",
       " '-paul',\n",
       " '-pearl',\n",
       " '-pearson',\n",
       " '-physically',\n",
       " '-plato',\n",
       " '-platomale',\n",
       " '-play',\n",
       " '-please',\n",
       " '-powerful',\n",
       " '-practice',\n",
       " '-preferably',\n",
       " '-president',\n",
       " '-priced',\n",
       " '-print',\n",
       " '-prompt',\n",
       " '-proverbdue',\n",
       " '-pythagoras',\n",
       " '-r.j',\n",
       " '-rachel',\n",
       " '-raffi',\n",
       " '-raising',\n",
       " '-ralph',\n",
       " '-rarely',\n",
       " '-rasmenia',\n",
       " '-ray',\n",
       " '-read',\n",
       " '-reading',\n",
       " '-regie',\n",
       " '-regular',\n",
       " '-related',\n",
       " '-rene',\n",
       " '-research',\n",
       " '-richard',\n",
       " '-right',\n",
       " '-risk',\n",
       " '-rita',\n",
       " '-roald',\n",
       " '-robert',\n",
       " '-ron',\n",
       " '-roughly',\n",
       " '-rumithen',\n",
       " '-run',\n",
       " '-s',\n",
       " '-s-',\n",
       " '-scholastic',\n",
       " '-scholasticthis',\n",
       " '-science',\n",
       " '-scott',\n",
       " '-second',\n",
       " '-senecaurban',\n",
       " '-share',\n",
       " '-shel',\n",
       " '-shelia',\n",
       " '-sherman',\n",
       " '-shoemaker',\n",
       " '-sir',\n",
       " '-sister',\n",
       " '-sixth',\n",
       " '-sized',\n",
       " '-small',\n",
       " '-so',\n",
       " '-socioeconomic',\n",
       " '-socrates',\n",
       " \"-socrates.i'd\",\n",
       " '-solving',\n",
       " '-sometimes',\n",
       " '-special',\n",
       " '-spence',\n",
       " '-staff',\n",
       " '-stans',\n",
       " '-step',\n",
       " '-stephanie',\n",
       " '-stephen',\n",
       " '-steve',\n",
       " '-students',\n",
       " '-susan',\n",
       " '-t',\n",
       " '-tagorea',\n",
       " '-take',\n",
       " '-teach',\n",
       " '-teachers',\n",
       " '-teaching',\n",
       " '-team',\n",
       " '-technology',\n",
       " '-temple',\n",
       " '-text',\n",
       " '-thank',\n",
       " '-that',\n",
       " '-the',\n",
       " '-then',\n",
       " '-theodore',\n",
       " '-there',\n",
       " '-these',\n",
       " '-they',\n",
       " '-thomas',\n",
       " '-threatening',\n",
       " '-three',\n",
       " '-tiny',\n",
       " '-to',\n",
       " '-tomie',\n",
       " '-trey',\n",
       " '-try',\n",
       " '-u.s',\n",
       " '-unknown',\n",
       " '-unknowndessert',\n",
       " '-up',\n",
       " '-us',\n",
       " '-used',\n",
       " '-value',\n",
       " '-vi',\n",
       " '-victor',\n",
       " '-vince',\n",
       " '-virtual',\n",
       " '-voltaireparents',\n",
       " '-w',\n",
       " '-w.b',\n",
       " '-walt',\n",
       " '-water',\n",
       " '-we',\n",
       " '-what',\n",
       " '-where',\n",
       " '-which',\n",
       " '-who',\n",
       " '-wide',\n",
       " '-will',\n",
       " '-william',\n",
       " '-winning',\n",
       " '-winston',\n",
       " '-with',\n",
       " '-woody',\n",
       " '-working',\n",
       " '-world',\n",
       " '-writing',\n",
       " '-year',\n",
       " '-yes',\n",
       " '-yet',\n",
       " '-yeuzheng',\n",
       " '-yoko',\n",
       " '-your',\n",
       " '-|',\n",
       " '.-',\n",
       " '.--',\n",
       " '.---',\n",
       " '.-drumming',\n",
       " '..',\n",
       " '...',\n",
       " '....',\n",
       " '.....',\n",
       " '......',\n",
       " '.......',\n",
       " '........',\n",
       " '.........',\n",
       " '..........',\n",
       " '...........',\n",
       " '............',\n",
       " '.............',\n",
       " '.01',\n",
       " '.04',\n",
       " '.05',\n",
       " '.1',\n",
       " '.18',\n",
       " '.20',\n",
       " '.22',\n",
       " '.25',\n",
       " '.3',\n",
       " '.4',\n",
       " '.45',\n",
       " '.5',\n",
       " '.583',\n",
       " '.6',\n",
       " '.7',\n",
       " '.75',\n",
       " '.82',\n",
       " '.95',\n",
       " '.a',\n",
       " '.absolutely',\n",
       " '.after',\n",
       " '.allowing',\n",
       " '.also',\n",
       " '.although',\n",
       " '.and',\n",
       " '.another',\n",
       " '.are',\n",
       " '.as',\n",
       " '.at',\n",
       " '.awesome',\n",
       " '.bee',\n",
       " '.but',\n",
       " '.by',\n",
       " '.challenging',\n",
       " '.clarence',\n",
       " '.distances',\n",
       " '.do',\n",
       " '.english',\n",
       " '.enrichment',\n",
       " '.find',\n",
       " '.finding',\n",
       " '.fitness',\n",
       " '.get',\n",
       " '.help',\n",
       " '.helping',\n",
       " '.hook',\n",
       " '.hopefully',\n",
       " '.i',\n",
       " '.in',\n",
       " '.inside',\n",
       " '.instead',\n",
       " '.is',\n",
       " '.it',\n",
       " '.jpeg',\n",
       " '.last',\n",
       " '.many',\n",
       " '.multimedia',\n",
       " '.my',\n",
       " '.not',\n",
       " '.now',\n",
       " '.oh',\n",
       " '.our',\n",
       " '.pdf',\n",
       " '.please',\n",
       " '.practicing',\n",
       " '.pro',\n",
       " '.put',\n",
       " '.reading',\n",
       " '.ready',\n",
       " '.resulting',\n",
       " '.reusable',\n",
       " '.school',\n",
       " '.seriously',\n",
       " '.share',\n",
       " '.since',\n",
       " '.speaker',\n",
       " '.stay',\n",
       " '.stories',\n",
       " '.struggle',\n",
       " '.students',\n",
       " '.that',\n",
       " '.the',\n",
       " '.these',\n",
       " '.they',\n",
       " '.this',\n",
       " '.through',\n",
       " '.unfortunately',\n",
       " '.values',\n",
       " '.videos',\n",
       " '.we',\n",
       " '.working',\n",
       " '.writing',\n",
       " '.~',\n",
       " '/',\n",
       " '/-',\n",
       " '/8th',\n",
       " '/a/',\n",
       " '/activity',\n",
       " '/adequate',\n",
       " '/ap',\n",
       " '/apple',\n",
       " '/at/',\n",
       " '/b/',\n",
       " '/c/',\n",
       " '/ch/).with',\n",
       " '/colored',\n",
       " '/contrast',\n",
       " '/d/',\n",
       " '/deaf',\n",
       " '/disney',\n",
       " '/e/',\n",
       " '/experience',\n",
       " '/f/',\n",
       " '/felt',\n",
       " '/further',\n",
       " '/if',\n",
       " '/is',\n",
       " '/lessons',\n",
       " '/m/',\n",
       " '/my',\n",
       " '/no/.',\n",
       " '/non',\n",
       " '/on/',\n",
       " '/or',\n",
       " '/p/',\n",
       " '/pe',\n",
       " '/personal',\n",
       " '/planets',\n",
       " '/point',\n",
       " '/qu/.',\n",
       " '/r/',\n",
       " '/r/,/s/',\n",
       " '/s.i.m.s',\n",
       " '/s/',\n",
       " '/science',\n",
       " '/sims',\n",
       " '/study',\n",
       " '/t/',\n",
       " '/t/.',\n",
       " '/t/?my',\n",
       " '/teacher',\n",
       " '/understanding',\n",
       " '/writing',\n",
       " '0',\n",
       " '0-',\n",
       " '0.001',\n",
       " '0.01',\n",
       " '0.04',\n",
       " '0.05',\n",
       " '0.1',\n",
       " '0.1-',\n",
       " '0.2',\n",
       " '0.20',\n",
       " '0.3',\n",
       " '0.35',\n",
       " '0.4',\n",
       " '0.5',\n",
       " '0.50',\n",
       " '0.58',\n",
       " '0.6',\n",
       " '0.6-',\n",
       " '0.60',\n",
       " '0.661',\n",
       " '0.7-',\n",
       " '0.75',\n",
       " '0.8',\n",
       " '0.85',\n",
       " '0.9',\n",
       " '0.95',\n",
       " '0/6',\n",
       " '00',\n",
       " '000',\n",
       " '001please',\n",
       " '00s',\n",
       " '01',\n",
       " '02',\n",
       " '03',\n",
       " '055x',\n",
       " '06',\n",
       " '08',\n",
       " '09',\n",
       " '0=3',\n",
       " '0f',\n",
       " '0we',\n",
       " '1',\n",
       " '1!one',\n",
       " '1)engage',\n",
       " '1+x=10',\n",
       " '1,000',\n",
       " '1,000,000',\n",
       " '1,000,000,000',\n",
       " '1,000s',\n",
       " '1,008',\n",
       " '1,011',\n",
       " '1,012',\n",
       " '1,025',\n",
       " '1,030',\n",
       " '1,033',\n",
       " '1,045',\n",
       " '1,050',\n",
       " '1,067',\n",
       " '1,076',\n",
       " '1,098',\n",
       " '1,100',\n",
       " '1,100(professional',\n",
       " '1,106',\n",
       " '1,125',\n",
       " '1,144',\n",
       " '1,150',\n",
       " '1,160',\n",
       " '1,169',\n",
       " '1,2',\n",
       " '1,2,3',\n",
       " '1,2,3,4',\n",
       " '1,2,3-read',\n",
       " '1,2,4,and',\n",
       " '1,200',\n",
       " '1,230',\n",
       " '1,240',\n",
       " '1,245',\n",
       " '1,250',\n",
       " '1,256',\n",
       " '1,260',\n",
       " '1,3,and',\n",
       " '1,300',\n",
       " '1,334',\n",
       " '1,350',\n",
       " '1,358',\n",
       " '1,361',\n",
       " '1,400',\n",
       " '1,440',\n",
       " '1,450',\n",
       " '1,470',\n",
       " '1,480',\n",
       " '1,5',\n",
       " '1,500',\n",
       " '1,500.we',\n",
       " '1,517',\n",
       " '1,551',\n",
       " '1,600',\n",
       " '1,637',\n",
       " '1,650',\n",
       " '1,700',\n",
       " '1,750',\n",
       " '1,800',\n",
       " '1,800,000',\n",
       " '1,820',\n",
       " '1,840',\n",
       " '1,900',\n",
       " '1,944',\n",
       " '1,950',\n",
       " '1-',\n",
       " '1-3.here',\n",
       " '1-5.they',\n",
       " '1-a',\n",
       " '1-bedroom',\n",
       " '1-create',\n",
       " '1-d',\n",
       " '1-hour',\n",
       " '1-inch',\n",
       " '1-informational',\n",
       " '1-mile',\n",
       " '1-minute',\n",
       " '1-music',\n",
       " '1-on-1',\n",
       " '1-parent',\n",
       " '1-piece',\n",
       " '1-point',\n",
       " '1-read',\n",
       " '1-to-1',\n",
       " '1-to-3',\n",
       " '1-year',\n",
       " '1.0',\n",
       " '1.00',\n",
       " '1.1',\n",
       " '1.16',\n",
       " '1.2',\n",
       " '1.2.3',\n",
       " '1.25',\n",
       " '1.3',\n",
       " '1.4',\n",
       " '1.48',\n",
       " '1.5',\n",
       " '1.5).i',\n",
       " '1.54',\n",
       " '1.5v',\n",
       " '1.6',\n",
       " '1.62',\n",
       " '1.66',\n",
       " '1.7',\n",
       " '1.7.1',\n",
       " '1.75',\n",
       " '1.77',\n",
       " '1.77%)so',\n",
       " '1.8',\n",
       " '1.8.1',\n",
       " '1.9',\n",
       " '1.community',\n",
       " '1.i',\n",
       " '1.my',\n",
       " '1.small',\n",
       " '1.the',\n",
       " '1.what',\n",
       " '1/',\n",
       " '1/10',\n",
       " '1/12',\n",
       " '1/16',\n",
       " '1/17',\n",
       " '1/2',\n",
       " '1/2-month',\n",
       " '1/2-size',\n",
       " '1/25',\n",
       " '1/2x11',\n",
       " '1/3',\n",
       " '1/3rd',\n",
       " '1/4',\n",
       " '1/4-size',\n",
       " '1/4th',\n",
       " '1/5',\n",
       " '1/5th',\n",
       " '1/6th',\n",
       " '1/7/13',\n",
       " '1/8',\n",
       " '1/8000s',\n",
       " '1/equity',\n",
       " '1/low',\n",
       " '1/program',\n",
       " '1/title',\n",
       " '10',\n",
       " '10!.',\n",
       " '10!i',\n",
       " '10%+',\n",
       " '10%-black',\n",
       " ...]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "a19b35a3-78f5-4122-8d45-bd6dde4e26ed"
    }
   },
   "source": [
    "# Unrelated Stuff / Testing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "f1df4d32-8b08-40eb-81fc-24d300191c87"
    }
   },
   "source": [
    "## Understanding tfidf with small examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "828f2b11-aa1a-4ccd-9586-215ae5a349cc"
    }
   },
   "outputs": [],
   "source": [
    "# Good intro with nice code: https://towardsdatascience.com/hacking-scikit-learns-vectorizers-9ef26a7170af\n",
    "# create a dataframe from a word matrix\n",
    "def wm2df(wm, feat_names):\n",
    "    \n",
    "    # create an index for each row\n",
    "    timeit doc_names = ['Doc{:d}'.format(idx) for idx, _ in enumerate(wm)]\n",
    "    df = pd.DataFrame(data=wm.toarray(), index=doc_names,\n",
    "                      columns=feat_names)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "b1a99f01-d40d-4bf1-a21f-396d548de816"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "test_series = pd.Series([\"I write a sentence about school's computer \", \"I've written a sentence about school furniture\", \"I am writing a sentence about school computer periphery\"])\n",
    "\n",
    "# smooth_idf adds one to every df to prevent division by \"0\"\n",
    "# Create spacy tokenzier\n",
    "spacy.load(\"en\")\n",
    "lemmatizer = spacy.lang.en.English()\n",
    "def spacy_tokenizer(doc):\n",
    "    tokens = lemmatizer(doc)\n",
    "    return([token.lemma_ for token in tokens])\n",
    "# Vectorize with custom spacy token lemmatizer, e.g. written, writing --> write\n",
    "test_tfidf = TfidfVectorizer(tokenizer=spacy_tokenizer, analyzer=\"word\", stop_words=\"english\", smooth_idf=False)\n",
    "test_fitted = test_tfidf.fit(test_series)\n",
    "test_transformed = test_fitted.transform(test_series)\n",
    "\n",
    "idf = test_tfidf.idf_\n",
    "\n",
    "print(\"> The vocabulary doesn't contain 'write' and 'written' as seperate tokens. '-PRON-' is a special token for pronouns\")\n",
    "print(test_tfidf.vocabulary_)\n",
    "print(\"\\n> 'Furniture' and periphery have the highest idf weights because they only appear in one document. 'Computer' appears in two out of three documents\")\n",
    "# Zip groups the elements from first object with elements from second object by index\n",
    "print(dict(zip(test_fitted.get_feature_names(), idf)))\n",
    "print(\"\\n> The tf-idf matrix\")\n",
    "print(wm2df(test_transformed, test_tfidf.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "377d5870-77c2-4ca0-9979-06eedb83490c"
    }
   },
   "source": [
    "## Nice performance evaluation tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "e38319eb-23d0-44a5-9021-085e7d44439b"
    }
   },
   "outputs": [],
   "source": [
    "def my_add():\n",
    "    return list(projects.loc[:, \"Teacher Project Posted Sequence\"] + 100)\n",
    "\n",
    "def iter_add():\n",
    "    lst = []\n",
    "    for row in projects.loc[:,\"Teacher Project Posted Sequence\"].values:\n",
    "        val = row + 100\n",
    "        lst.append(val)\n",
    "    return  lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "7e05d63e-39ea-4f9a-ae8a-5d147b7b012b"
    }
   },
   "outputs": [],
   "source": [
    "%timeit my_add()\n",
    "%timeit iter_add()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "nbpresent": {
     "id": "eb035902-8eb9-4db9-8cb7-0719668f9408"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "c696d017-d996-47cc-9cab-b564e7bfb9ac"
    }
   },
   "outputs": [],
   "source": [
    "%lprun -f my_add my_add()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "6d0c6b7f-1d25-4ea1-8d3b-5d5a3782f110"
    }
   },
   "outputs": [],
   "source": [
    "%lprun -f iter_add iter_add()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
